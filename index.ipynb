{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head\n",
    "* so in transformer we do use multiplicative attention ,there are two type of attention one is additive and other is scale dot attention\n",
    "* so in research paper  \"attention all u need\" they hav incorported multiplicative attention\n",
    "* i assume it just involves addition rather than the multiplication to calculate the score\n",
    "\n",
    "dimension of data (directly proportional to) variance \n",
    "\n",
    "hence the scailing down is down by (dimension of model)**(1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#it has been modified to process the 3d data where the data across the z index correspond to each head\n",
    "\n",
    "def attention_head(queries, keys, values, dropout=None, mask=None):\n",
    "\n",
    "  # Calculate the attention scores\n",
    "  # Scores will have shape (batch_size * sequence_length, batch_size * sequence_length)\n",
    "  d_k = keys.size(-1)\n",
    "  scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "  scores = scores / d_k**0.5\n",
    "\n",
    "#now this look ahead mask will be used in the decoder\n",
    "  if mask is not None:\n",
    "# mask should be broadcastable to the shape of the scores tensor\n",
    "# for 3d input, the mask should have shape (num_heads,batch_size * sequence_length, batch_size * sequence_length)\n",
    "      scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "  # row mean -1 and -2 means col\n",
    "  scores = scores.softmax(dim=-1) # apply softmax across the row dimension (keys)\n",
    "\n",
    "\n",
    "  # Apply dropout\n",
    "  if dropout is not None:\n",
    "      scores = dropout(scores)\n",
    "\n",
    "  # Apply scores to values\n",
    "  weighted_sum = torch.matmul(scores, values)\n",
    "\n",
    "  return weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will test our attention head with custom args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define input dimensions\n",
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "embedding_dim = 4\n",
    "num_heads = 2\n",
    "head_dim = embedding_dim // num_heads\n",
    "\n",
    "# Create sample input tensors with compatible shapes\n",
    "# Embeddings are now 2D: (batch_size * sequence_length, embedding_dim)\n",
    "sample_embeddings = torch.randn(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "# Create dummy linear layers that output the expected shape for attention_head\n",
    "# The linear layers now expect an input shape of (batch_size * sequence_length, embedding_dim)\n",
    "# and output a shape of (batch_size * sequence_length, head_dim)\n",
    "dummy_key_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "dummy_query_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "dummy_value_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "# Create a dummy dropout layer\n",
    "dummy_dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "# Create a dummy mask\n",
    "# The mask should have shape (batch_size * sequence_length, batch_size * sequence_length) for 2D input\n",
    "dummy_mask = torch.ones(batch_size * sequence_length, batch_size * sequence_length) # Assuming no masking for simplicity\n",
    "\n",
    "# Pass the sample_embeddings through the dummy linear layers to get the keys, queries, and values tensors\n",
    "# These tensors should have shape (batch_size * sequence_length, head_dim)\n",
    "sample_keys = dummy_key_linear(sample_embeddings)\n",
    "sample_queries = dummy_query_linear(sample_embeddings)\n",
    "sample_values = dummy_value_linear(sample_embeddings)\n",
    "\n",
    "print(\"Sample Keys Shape:\", sample_keys.shape)\n",
    "print(\"Sample Queries Shape:\", sample_queries.shape)\n",
    "print(\"Sample Values Shape:\", sample_values.shape)\n",
    "\n",
    "# Test the attention_head function with the compatible inputs\n",
    "\n",
    "  # attention_head now expects 2D inputs for keys, queries, and values\n",
    "attention_output = attention_head(sample_queries, sample_keys, sample_values, dummy_dropout, dummy_mask)\n",
    "print(\"Attention head output shape:\", attention_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing for the 3d inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define input dimensions\n",
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "embedding_dim = 4\n",
    "num_heads = 2\n",
    "head_dim = embedding_dim // num_heads\n",
    "\n",
    "# Create sample input tensors with compatible shapes\n",
    "# Embeddings are now 2D: (batch_size * sequence_length, embedding_dim)\n",
    "sample_embeddings = torch.randn(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "# Create dummy linear layers that output the expected shape for attention_head\n",
    "# The linear layers now expect an input shape of (batch_size * sequence_length, embedding_dim)\n",
    "# and output a shape of (batch_size * sequence_length, head_dim)\n",
    "dummy_key_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "dummy_query_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "dummy_value_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "# Create a dummy dropout layer\n",
    "dummy_dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "# Create a dummy mask\n",
    "# The mask should have shape (batch_size * sequence_length, batch_size * sequence_length) for 2D input\n",
    "dummy_mask = torch.ones(batch_size * sequence_length, batch_size * sequence_length) # Assuming no masking for simplicity\n",
    "\n",
    "# Pass the sample_embeddings through the dummy linear layers to get the keys, queries, and values tensors\n",
    "# These tensors should have shape (batch_size * sequence_length, head_dim)\n",
    "sample_keys = dummy_key_linear(sample_embeddings)\n",
    "sample_queries = dummy_query_linear(sample_embeddings)\n",
    "sample_values = dummy_value_linear(sample_embeddings)\n",
    "\n",
    "print(\"Sample Keys Shape:\", sample_keys.shape)\n",
    "print(\"Sample Queries Shape:\", sample_queries.shape)\n",
    "print(\"Sample Values Shape:\", sample_values.shape)\n",
    "\n",
    "# Test the attention_head function with the compatible inputs\n",
    "\n",
    "  # attention_head now expects 2D inputs for keys, queries, and values\n",
    "attention_output = attention_head(sample_queries, sample_keys, sample_values, dummy_dropout, dummy_mask)\n",
    "print(\"Attention head output shape:\", attention_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention \n",
    "#### we create tunable parameters for each head one for query ,key and value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention flow](./attention_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self,heads,d_model,dropout=0.01):\n",
    "     super(MultiHeadAttention,self).__init__();\n",
    "     self.heads=heads;\n",
    "     self.d_model=d_model;\n",
    "     self.d_key=d_model//heads;\n",
    "     # Linear layers for query, key, value, and projection\n",
    "     self.query_linears = nn.Linear(d_model, d_model)\n",
    "     self.key_linears =   nn.Linear(d_model, d_model)\n",
    "     self.value_linears = nn.Linear(d_model, d_model)\n",
    "     #prjection layer will be used to project the concatenated output of all heads to the end of attention in the multihead attention\n",
    "     self.projection_layer=nn.Linear(d_model,self.d_model)\n",
    "     #this field is for debugging purposes\n",
    "     self.attn=None\n",
    "     self.dropout=nn.Dropout(dropout);\n",
    "\n",
    "  def forward(self,input_embedding):\n",
    "    head_dim=self.d_model//self.heads;\n",
    "    queries=self.query_linears(input_embedding).view(self.heads,-1,head_dim);\n",
    "    keys=self.key_linears(input_embedding).view(self.heads,-1,head_dim);\n",
    "    values=self.value_linears(input_embedding).view(self.heads,-1,head_dim);\n",
    "    dropout = torch.nn.Dropout(0.1)\n",
    "    result=attention_head(queries,keys,values,dropout=dropout).view(-1,self.d_model)\n",
    "    return self.projection_layer(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define parameters for MultiHeadAttention\n",
    "heads = 4\n",
    "d_model = 128 # Make sure d_model is divisible by heads\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# Create a dummy input embedding\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "input_embedding = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "# Create an instance of MultiHeadAttention\n",
    "multi_head_attention = MultiHeadAttention(heads, d_model, dropout=dropout_prob)\n",
    "\n",
    "# Test the forward method\n",
    "output = multi_head_attention(input_embedding)\n",
    "\n",
    "print(\"Input embedding shape:\", input_embedding.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the add and normalize layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# this add and normalize layer will be used in the subsidual linkage \n",
    "# this subisidual is used to acoid the diminishing effect or influence of the original input\n",
    "def add_norm(prev,input):\n",
    "  result=prev+input;\n",
    "  return torch.nn.functional.normalize(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the position wise feed forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "  def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "    super(PositionWiseFFN,self).__init__();\n",
    "    self.linear1=nn.Linear(d_model,d_ff);\n",
    "    self.linear2=nn.Linear(d_ff,d_model);\n",
    "    self.dropout=nn.Dropout(dropout);\n",
    "\n",
    "  def forward(self,x):\n",
    "    x=self.linear1(x);\n",
    "    x=torch.relu(x);\n",
    "    x=self.dropout(x);\n",
    "    x=self.linear2(x);\n",
    "    return x;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test case for the position wise feed forward network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test case for PositionWiseFFN\n",
    "\n",
    "# Define input dimensions\n",
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "embedding_dim = 4\n",
    "d_ff = 8 # Dimension of the feed-forward hidden layer\n",
    "\n",
    "# Create sample input tensor with shape (batch_size * sequence_length, embedding_dim)\n",
    "sample_input = torch.randn(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "print(\"Sample Input Shape for PositionWiseFFN:\", sample_input.shape)\n",
    "\n",
    "# Instantiate the PositionWiseFFN module\n",
    "position_wise_ffn = PositionWiseFFN(d_model=embedding_dim, d_ff=d_ff)\n",
    "\n",
    "# Pass the sample input through the PositionWiseFFN module\n",
    "try:\n",
    "  print(sample_input)\n",
    "  output_ffn = position_wise_ffn(sample_input)\n",
    "  print(\"PositionWiseFFN Output Shape:\", output_ffn)\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred during the PositionWiseFFN test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_position_encoding(seq_len, d_model, n=10000): \n",
    "\n",
    "    P = torch.zeros((seq_len, d_model)) \n",
    "\n",
    "    for pos in range(seq_len): \n",
    "\n",
    "        for i in range(d_model): \n",
    "\n",
    "            angle = pos / np.power(n, (2 * (i // 2)) / d_model) \n",
    "\n",
    "            P[pos, i] = np.sin(angle) if i % 2 == 0 else np.cos(angle) \n",
    "\n",
    "    return P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test case for get_position_encoding\n",
    "\n",
    "# Define parameters for testing\n",
    "seq_len = 1\n",
    "d_model = 10\n",
    "\n",
    "# Get positional encoding\n",
    "position_encoding = get_position_encoding(seq_len, d_model)\n",
    "\n",
    "print(\"Position Encoding Shape:\", position_encoding.shape)\n",
    "print(\"Position Encoding:\\n\", position_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define parameters (you can adjust these)\n",
    "seq_len = 50\n",
    "d_model = 128\n",
    "\n",
    "# Get positional encoding\n",
    "position_encoding = get_position_encoding(seq_len, d_model)\n",
    "\n",
    "# Visualize the positional encoding\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(position_encoding.numpy(), cmap=\"viridis\")\n",
    "plt.title(\"Positional Encoding Heatmap\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Position in Sequence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating the add and normalisation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def add_norm(prev,input):\n",
    "  result=prev+input;\n",
    "  return torch.nn.functional.normalize(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Encoder Sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderSublayer(nn.Module):\n",
    "  def __init__(self,d_model,num_heads,ffn_dim) -> None:\n",
    "    super(EncoderSublayer,self).__init__()\n",
    "    self.multihead=MultiHeadAttention(heads=num_heads,d_model=d_model)\n",
    "    self.ffn=PositionWiseFFN(d_model=d_model,d_ff=ffn_dim)\n",
    "    self.d_model=d_model\n",
    "    self.ffn_dim=ffn_dim\n",
    "  def forward(self,input_embeddings):\n",
    "    add_norm_1=add_norm(prev=input_embeddings,input=self.multihead(input_embeddings))\n",
    "    add_norm_2=add_norm(prev=add_norm_1,input=self.ffn(add_norm_1))\n",
    "    return add_norm_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the test case for the encoder sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test case for EncoderSublayer\n",
    "\n",
    "# Define input dimensions\n",
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "embedding_dim = 4\n",
    "num_heads = 2\n",
    "ffn_dim = 8 # Dimension of the feed-forward hidden layer\n",
    "\n",
    "# Create sample input embeddings with shape (batch_size * sequence_length, embedding_dim)\n",
    "sample_input_embeddings = torch.randn(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "print(\"Sample Input Embeddings Shape for EncoderSublayer:\", sample_input_embeddings.shape)\n",
    "\n",
    "# Instantiate the EncoderSublayer module\n",
    "encoder_sublayer = EncoderSublayer(d_model=embedding_dim, num_heads=num_heads, ffn_dim=ffn_dim)\n",
    "\n",
    "# Pass the sample input embeddings through the EncoderSublayer module\n",
    "try:\n",
    "  output_encoder_sublayer = encoder_sublayer(sample_input_embeddings)\n",
    "  print(\"EncoderSublayer Output Shape:\", output_encoder_sublayer.shape)\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred during the EncoderSublayer test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,d_model,num_heads,ffn_dim,num_layers,n=None) -> None:\n",
    "    super(Encoder,self).__init__()\n",
    "    self.num_heads=num_heads\n",
    "    self.num_layers=num_layers\n",
    "    self.d_model=d_model\n",
    "    self.ffn_dim=ffn_dim\n",
    "    self.n=n\n",
    "    self.encoder_layers=[EncoderSublayer(d_model=d_model,num_heads=num_heads,ffn_dim=ffn_dim) for i in range(0,num_layers)];\n",
    "  def forward(self,input_embedding):\n",
    "    pe=None\n",
    "    if (self.n!=None):\n",
    "     pe=get_position_encoding(input_embedding.size(0),self.d_model,self.n)\n",
    "    else:\n",
    "     pe=get_position_encoding(input_embedding.size(0),self.d_model)\n",
    "    print(\"positional encoding\",pe.shape)\n",
    "    print(\"input embedding\",input_embedding.shape)\n",
    "\n",
    "    result=add_norm(prev=input_embedding,input=pe)\n",
    "    for i in range(self.num_layers):\n",
    "      result=self.encoder_layers[i](result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the test case for the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test case for Encoder\n",
    "\n",
    "# Define input dimensions\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "d_model = 128 # Make sure d_model is divisible by num_heads\n",
    "num_heads = 4\n",
    "ffn_dim = 256 # Dimension of the feed-forward hidden layer\n",
    "num_layers = 2 # Number of encoder layers\n",
    "\n",
    "# Create sample input embeddings with shape (batch_size, sequence_length, d_model)\n",
    "sample_input_embeddings = torch.randn(batch_size* sequence_length, d_model)\n",
    "\n",
    "print(\"Sample Input Embeddings Shape for Encoder:\", sample_input_embeddings.shape)\n",
    "\n",
    "# Instantiate the Encoder module\n",
    "encoder = Encoder(d_model=d_model, num_heads=num_heads, ffn_dim=ffn_dim, num_layers=num_layers)\n",
    "\n",
    "# Pass the sample input embeddings through the Encoder module\n",
    "\n",
    "output_encoder = encoder(sample_input_embeddings)\n",
    "print(\"Encoder Output Shape:\", output_encoder.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
