{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head\n",
    "* so in transformer we do use multiplicative attention ,there are two type of attention one is additive and other is scale dot attention\n",
    "* so in research paper  \"attention all u need\" they hav incorported multiplicative attention\n",
    "* i assume it just involves addition rather than the multiplication to calculate the score\n",
    "\n",
    "dimension of data (directly proportional to) variance \n",
    "\n",
    "hence the scailing down is down by (dimension of model)**(1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#it has been modified to process the 3d data where the data across the z index correspond to each head\n",
    "\n",
    "def attention_head(queries, keys, values, dropout=None, mask=None):\n",
    "\n",
    "  # Calculate the attention scores\n",
    "  # Scores will have shape (batch_size * sequence_length, batch_size * sequence_length)\n",
    "  d_k = keys.size(-1)\n",
    "  scores = nn.matmul(queries, keys.transpose(-2, -1))\n",
    "  scores = scores / d_k**0.5\n",
    "\n",
    "#now this look ahead mask will be used in the decoder\n",
    "  if mask is not None:\n",
    "# mask should be broadcastable to the shape of the scores tensor\n",
    "# for 3d input, the mask should have shape (num_heads,batch_size * sequence_length, batch_size * sequence_length)\n",
    "      scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "  # row mean -1 and -2 means col\n",
    "  scores = scores.softmax(dim=-1) # apply softmax across the row dimension (keys)\n",
    "\n",
    "\n",
    "  # Apply dropout\n",
    "  if dropout is not None:\n",
    "      scores = dropout(scores)\n",
    "\n",
    "  # Apply scores to values\n",
    "  weighted_sum = nn.matmul(scores, values)\n",
    "\n",
    "  return weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will test our attention head with custom args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define input dimensions\n",
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "embedding_dim = 4\n",
    "num_heads = 2\n",
    "head_dim = embedding_dim // num_heads\n",
    "\n",
    "# Create sample input tensors with compatible shapes\n",
    "# Embeddings are now 2D: (batch_size * sequence_length, embedding_dim)\n",
    "sample_embeddings = torch.randn(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "# Create dummy linear layers that output the expected shape for attention_head\n",
    "# The linear layers now expect an input shape of (batch_size * sequence_length, embedding_dim)\n",
    "# and output a shape of (batch_size * sequence_length, head_dim)\n",
    "dummy_key_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "dummy_query_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "dummy_value_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "# Create a dummy dropout layer\n",
    "dummy_dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "# Create a dummy mask\n",
    "# The mask should have shape (batch_size * sequence_length, batch_size * sequence_length) for 2D input\n",
    "dummy_mask = torch.ones(batch_size * sequence_length, batch_size * sequence_length) # Assuming no masking for simplicity\n",
    "\n",
    "# Pass the sample_embeddings through the dummy linear layers to get the keys, queries, and values tensors\n",
    "# These tensors should have shape (batch_size * sequence_length, head_dim)\n",
    "sample_keys = dummy_key_linear(sample_embeddings)\n",
    "sample_queries = dummy_query_linear(sample_embeddings)\n",
    "sample_values = dummy_value_linear(sample_embeddings)\n",
    "\n",
    "print(\"Sample Keys Shape:\", sample_keys.shape)\n",
    "print(\"Sample Queries Shape:\", sample_queries.shape)\n",
    "print(\"Sample Values Shape:\", sample_values.shape)\n",
    "\n",
    "# Test the attention_head function with the compatible inputs\n",
    "\n",
    "  # attention_head now expects 2D inputs for keys, queries, and values\n",
    "attention_output = attention_head(sample_queries, sample_keys, sample_values, dummy_dropout, dummy_mask)\n",
    "print(\"Attention head output shape:\", attention_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing for the 3d inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define input dimensions\n",
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "embedding_dim = 4\n",
    "num_heads = 2\n",
    "head_dim = embedding_dim // num_heads\n",
    "\n",
    "# Create sample input tensors with compatible shapes\n",
    "# Embeddings are now 2D: (batch_size * sequence_length, embedding_dim)\n",
    "sample_embeddings = torch.randn(batch_size * sequence_length, embedding_dim)\n",
    "\n",
    "# Create dummy linear layers that output the expected shape for attention_head\n",
    "# The linear layers now expect an input shape of (batch_size * sequence_length, embedding_dim)\n",
    "# and output a shape of (batch_size * sequence_length, head_dim)\n",
    "dummy_key_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "dummy_query_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "dummy_value_linear = torch.nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "# Create a dummy dropout layer\n",
    "dummy_dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "# Create a dummy mask\n",
    "# The mask should have shape (batch_size * sequence_length, batch_size * sequence_length) for 2D input\n",
    "dummy_mask = torch.ones(batch_size * sequence_length, batch_size * sequence_length) # Assuming no masking for simplicity\n",
    "\n",
    "# Pass the sample_embeddings through the dummy linear layers to get the keys, queries, and values tensors\n",
    "# These tensors should have shape (batch_size * sequence_length, head_dim)\n",
    "sample_keys = dummy_key_linear(sample_embeddings)\n",
    "sample_queries = dummy_query_linear(sample_embeddings)\n",
    "sample_values = dummy_value_linear(sample_embeddings)\n",
    "\n",
    "print(\"Sample Keys Shape:\", sample_keys.shape)\n",
    "print(\"Sample Queries Shape:\", sample_queries.shape)\n",
    "print(\"Sample Values Shape:\", sample_values.shape)\n",
    "\n",
    "# Test the attention_head function with the compatible inputs\n",
    "\n",
    "  # attention_head now expects 2D inputs for keys, queries, and values\n",
    "attention_output = attention_head(sample_queries, sample_keys, sample_values, dummy_dropout, dummy_mask)\n",
    "print(\"Attention head output shape:\", attention_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention \n",
    "#### we create tunable parameters for each head one for query ,key and value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self,heads,d_model,dropout=0.01):\n",
    "     super(MultiHeadAttention,self).__init__();\n",
    "     self.heads=heads;\n",
    "     self.d_model=d_model;\n",
    "     self.d_key=d_model//heads;\n",
    "     # Linear layers for query, key, value, and projection\n",
    "     self.query_linears = clones(nn.Linear(d_model, d_model//self.heads),self.heads)\n",
    "     self.key_linears =   clones(nn.Linear(d_model, d_model//self.heads),self.heads)\n",
    "     self.value_linears = clones(nn.Linear(d_model, d_model//self.heads),self.heads)\n",
    "     #prjection layer will be used to project the concatenated output of all heads to the end of attention in the multihead attention\n",
    "     self.projection_layer=nn.Linear(d_model,self.d_key)\n",
    "     self.dropout=nn.Dropout(dropout);\n",
    "\n",
    "  def forward(self,input_embedding):\n",
    "    batch_size = input_embedding.size(0)\n",
    "    result=[]\n",
    "    for i in range(0,self.heads):\n",
    "        out=attention_head(input_embedding, self.query_linears[i], self.key_linears[i], self.value_linears[i],self.dropout)\n",
    "        result.append(out)\n",
    "    final_result = torch.cat(result_list, dim=1)\n",
    "    return self.projection_layer(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define parameters for MultiHeadAttention\n",
    "heads = 4\n",
    "d_model = 128 # Make sure d_model is divisible by heads\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# Create a dummy input embedding\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "input_embedding = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "# Create an instance of MultiHeadAttention\n",
    "multi_head_attention = MultiHeadAttention(heads, d_model, dropout=dropout_prob)\n",
    "\n",
    "# Test the forward method\n",
    "output = multi_head_attention(input_embedding)\n",
    "\n",
    "print(\"Input embedding shape:\", input_embedding.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
