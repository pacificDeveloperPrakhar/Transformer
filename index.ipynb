{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head\n",
    "* so in transformer we do use multiplicative attention ,there are two type of attention one is additive and other is scale dot attention\n",
    "* so in research paper  \"attention all u need\" they hav incorported multiplicative attention\n",
    "* i assume it just involves addition rather than the multiplication to calculate the score\n",
    "\n",
    "dimension of data (directly proportional to) variance \n",
    "\n",
    "hence the scailing down is down by (dimension of model)**(1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def attention_head(embeddings,key_weights,query_weights,value_weights):\n",
    "  # first step is to calculate the keys,queries and value vectors for each embeddings\n",
    "  keys = nn.matmul(embeddings,key_weights)\n",
    "  queries = nn.matmul(embeddings,query_weights)\n",
    "  values = nn.matmul(embeddings,value_weights)\n",
    "  # now calculate the attention scores\n",
    "  result=[]\n",
    "  for i, query in enumerate(queries):\n",
    "    d_k = keys.size(-1)\n",
    "    # Calculate scores using matrix multiplication between query and transposed keys\n",
    "    scores = nn.matmul(query, keys.T)\n",
    "    scores = scores / d_k**0.5\n",
    "    # Apply scores to values using matrix multiplication\n",
    "    print(\"scores\",scores)\n",
    "    print(\"values\",values)\n",
    "    weighted_sum = nn.matmul(scores, values)\n",
    "    print(f\"Weighted sum for query {i}:\", weighted_sum)\n",
    "    result.append(weighted_sum)\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will test our attention head with custom args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define input embeddings and weights for testing with a single embedding\n",
    "embedding_dim = 4\n",
    "# Use the existing tensor_3d for testing\n",
    "embeddings_3d = tensor_3d.float() # Ensure the tensor is float\n",
    "key_weights = nn.randn(embedding_dim, embedding_dim)\n",
    "query_weights = nn.randn(embedding_dim, embedding_dim)\n",
    "value_weights = nn.randn(embedding_dim, embedding_dim)\n",
    "\n",
    "# Test the attention head with the 3 token embeddings\n",
    "output = attention_head(embeddings_3d, key_weights, query_weights, value_weights)\n",
    "print(\"Final output:\", output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
