{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head\n",
    "* so in transformer we do use multiplicative attention ,there are two type of attention one is additive and other is scale dot attention\n",
    "* so in research paper  \"attention all u need\" they hav incorported multiplicative attention\n",
    "* i assume it just involves addition rather than the multiplication to calculate the score\n",
    "\n",
    "dimension of data (directly proportional to) variance \n",
    "\n",
    "hence the scailing down is down by (dimension of model)**(1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def attention_head(embeddings,key_weights,query_weights,value_weights,dropout=None,mask=None):\n",
    "  # first step is to calculate the keys,queries and value vectors for each embeddings\n",
    "  keys = nn.matmul(embeddings,key_weights)\n",
    "  queries = nn.matmul(embeddings,query_weights)\n",
    "  values = nn.matmul(embeddings,value_weights)\n",
    "  # now calculate the attention scores\n",
    "  result=[]\n",
    "  for i, query in enumerate(queries):\n",
    "    d_k = keys.size(-1)\n",
    "    # Calculate scores using matrix multiplication between query and transposed keys\n",
    "    scores = nn.matmul(query, keys.T)\n",
    "    scores = scores / d_k**0.5\n",
    "    # replace 0 entries to negative infinite\n",
    "    if mask is not None:\n",
    "     scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    #do log softmax of the whole resulting score\n",
    "    scores=nn.nn.functional.log_softmax(scores,dim=0)\n",
    "    # now apply the dropout to ensure that the model weights are not dependent on one portion of data\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    # Apply scores to values using matrix multiplication\n",
    "    print(\"scores\",scores)\n",
    "    print(\"values\",values)\n",
    "    weighted_sum = nn.matmul(scores, values)\n",
    "    print(f\"Weighted sum for query {i}:\", weighted_sum)\n",
    "    result.append(weighted_sum)\n",
    "  return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will test our attention head with custom args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn_module\n",
    "import torch\n",
    "\n",
    "# Define input embeddings and weights for testing with a single embedding\n",
    "embedding_dim = 4\n",
    "# Use the existing tensor_3d for testing\n",
    "embeddings_3d = tensor_3d.float() # Ensure the tensor is float\n",
    "key_weights = nn.randn(embedding_dim, embedding_dim)\n",
    "query_weights = nn.randn(embedding_dim, embedding_dim)\n",
    "value_weights = nn.randn(embedding_dim, embedding_dim)\n",
    "\n",
    "# Define dropout layer for testing\n",
    "dropout_layer = nn_module.Dropout(p=0.1) # Example dropout probability of 0.1\n",
    "\n",
    "# Define a dummy mask for testing (replace with your actual mask logic)\n",
    "# This dummy mask will mask the second score for each query\n",
    "mask = torch.ones(embeddings_3d.size(0), embeddings_3d.size(0))\n",
    "mask[:, 1] = 0 # Example: mask out the second token for all queries\n",
    "\n",
    "\n",
    "# Test the attention head with the 3 token embeddings and dropout and mask\n",
    "output = attention_head(embeddings_3d, key_weights, query_weights, value_weights, dropout=dropout_layer, mask=mask)\n",
    "print(\"Final output:\", output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
